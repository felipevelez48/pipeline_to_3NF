# Dataâ€¯PipelineÂ Microâ€‘BatchesÂ ğŸ“¦â€¯âœâ€¯3â€¯NFÂ âœâ€¯TestsÂ âœâ€¯Airflow

![Pragma logo](images/pragma.jpg)

> **Reto tÃ©cnico â€“ Data Engineer**  
> Ingestamos un CSV masivo de ofertas de empleo, lo normalizamos en un modelo 3â€¯NF en PostgreSQL y orquestamos el ciclo completo con Dockerâ€¯Composeâ€¯+â€¯Airflow.  Incluimos pruebas de calidad con pytestâ€¯&â€¯pandera y un diseÃ±o conceptual OLAP.

---

## ğŸ“‚Â Estructura del repositorio

```text
.
â”œâ”€â”€ data/                    # âŠ CSV crudo (data_jobs.csv)
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ jobs_ingest_transform_dag.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schema.sql           # DDL 3â€¯NF + Ã­ndices
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ingest_raw.py        # carga data_jobs.csv â†’ raw.jobs
â”‚   â””â”€â”€ transform_to_3nf.py  # limpieza + 3â€¯NF
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_schema_raw.py   # Pandera
â”‚   â”œâ”€â”€ test_functions_cleaning.py
â”‚   â””â”€â”€ test_jobs_integrity.py
â”œâ”€â”€ docker-compose.yml       # PostgreSQL 16 + Airflow 2.9
â”œâ”€â”€ pyproject.toml           # Ruff config
â”œâ”€â”€ requirements.txt         # libs extra (ftfy, panderaâ€¦)
â””â”€â”€ README.md                # â† estÃ¡s aquÃ­
```


## âš™ï¸â€¯Requisitos

- Dockerâ€¯&â€¯DockerÂ Compose v2

- Make (opcional) para atajos (make up, make test, â€¦)

- No necesitas instalar nada en tu mÃ¡quina: todo corre en contenedores.

## ğŸš€â€¯Puesta en marcha rÃ¡pida
```bash
# 1) Copia variables de ejemplo y ajusta si quieres
cp .env.example .env.engineer

# 2) Construye imÃ¡genes e inicia servicios (â‰ˆ3â€‘4â€¯min la 1Âª vez)
docker compose up -d --build

# 3) Abre Airflow UI â†’ http://localhost:8080  (admin / admin)
#    Lanza la DAG jobs_ingest_transform (t1_ingest â†’ t2_transform)

# 4) Ejecuta pruebas de calidad
docker compose exec airflow pytest -q
# 8 tests OK  âœ…
```

## ğŸ—„ï¸â€¯Modelo relacional (3â€¯NF)
```bash
companies 1â”€â”
locations 1â”€â”¤
job_titles 1â”¤
schedules 1â”€â”´â”€â”
salary_rates 1â”˜ â”‚
                â”‚   (M:N)  skills
        jobsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ job_skill

```
- Columnas FK ahora son INTEGER y todas las claves forÃ¡neas se crearon (ver schema.sql).

- Ãndices extra en jobs(location_id) y jobs(company_id) aceleran las JOINs.

## ğŸ§ªâ€¯Tests con pytestâ€¯+â€¯pandera

| Test                             | QuÃ© valida                                              |
| -------------------------------- | ------------------------------------------------------- |
| **test\_schema\_raw\.py**        | El DF crudo respeta tipos y nulos.                      |
| **test\_functions\_cleaning.py** | Unit tests de `fix_encoding`, `clean_location`, etc.    |
| **test\_jobs\_integrity.py**     | *Checks* de FK + conteo de huÃ©rfanos (ahora todo pasa). |

## ğŸ”„â€¯OrquestaciÃ³n (Airflow)

- DAG: dags/jobs_ingest_transform_dag.py

    - t1_ingestâ€¯â†’â€¯lee CSV y llena raw.jobs (microâ€‘batches vÃ­a chunksize=1000).

    - t2_transformâ€¯â†’â€¯normaliza y carga 3â€¯NF (tambiÃ©n en lotes).

- Healthâ€‘check PostgreSQL con pg_isready evita carreras de arranque.

- Logs montados en ./logs para fÃ¡cil inspecciÃ³n.

## ğŸŒŸâ€¯DiseÃ±o Estrella (conceptual)

| Componente                  | DescripciÃ³n                                                     |
| --------------------------- | --------------------------------------------------------------- |
| **Fact** `fact_job_posting` | Grano: 1 oferta. FK: company, title, location, date.            |
| Dimâ€¯empresa                 | sector, tamaÃ±o, HQ.                                             |
| Dimâ€¯ubicaciÃ³n               | ciudad, estado, paÃ­s, lat/long.                                 |
| Dimâ€¯fecha                   | dÃ­a, mes, Q, festivo.                                           |
| Dimâ€¯tÃ­tulo                  | familia (Dataâ€¯Engineer, Analystâ€¦).                              |
| Dimâ€¯skill\*                 | con puente `fact_job_skill` para anÃ¡lisis de skills.            |
| **Medidas**                 | `salary_year_avg`, `salary_hour_avg`, `is_remote`, `days_open`. |

## ğŸ“â€¯Decisiones de diseÃ±o

- Microâ€‘batches con chunksize=1000 evitan OOM en portÃ¡tiles de 8â€‘16â€¯GB (dataset ~780â€¯k rows).

- ftfy corrige mojibake UTFâ€‘8/latinâ€‘1 (â€œROCKENÃ‚Â®â€ â†’ â€œROCKENÂ®â€).

- PIP_ADDITIONAL_REQUIREMENTS agiliza prototipos; para producciÃ³n se construirÃ­a imagen propia.

- Ruff elegido por rendimiento (â‰ˆ10Ã—â€¯Flake8).

- Pandera permite contratos de datos declarativos en puro Python.

- No se hardcodean credenciales; .env.engineer se monta vÃ­a env_file:.

## ğŸ’­â€¯Retos & aprendizajes

- **Tipado flotante accidental** â€“ pandas.to_sql convertÃ­a ids a float; los tests de FK fallaron y obligaron a castear a INTEGER.

- **Zombie tasks** â€“ Airflow marcaba tareas como â€œzombieâ€ por timeouts al procesar 12â€¯Mâ€¯rows; chunking y method="multi" lo resolvieron.

- **Memoria** â€“ El DataFrame completo no cabe; procesar en streaming fue clave.

- **Test first** â€“ Ver el rojo de pytest me guio paso a paso hasta la base limpia.

- **Historia personal** â€“ DescubrÃ­ lo fÃ¡cil que es sobreestimar la RAM de un portÃ¡til; la ejecuciÃ³n fallaba a la mitad y aprendÃ­ a dejar de sobreestimar la memoria RAM, la soluciÃ³n fue realizar chunks mÃ¡s pequeÃ±os que el equipo pudiera soportar.

## â–¶ï¸â€¯CÃ³mo probar otra carga
```bash
# 1. Copia otro CSV grande a data/ y renombra el anterior si no quieres duplicar.
# 2. Lanza nuevamente la DAG desde la UI o por CLI:
docker compose exec airflow airflow dags trigger jobs_ingest_transform
# 3. Reâ€‘ejecuta tests:
docker compose exec airflow pytest -q
```



## ğŸ“ŠğŸ¤– Autor: John Felipe VÃ©lez

### Data Engineer

